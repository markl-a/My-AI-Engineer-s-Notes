{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JJEH7ESvWA-"
      },
      "source": [
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Fret-KoXuw3Y0dev5IzysbRaRsSs0m0r?usp=sharing)\n",
        "\n",
        "(聲明：以下內容都是在網路上整理並修改的，真正我原創的內容並不多，我主要只是搬運工)\n",
        "\n",
        "這部分其實官方還在開發中，所以你可以預期連範例也可能會有bug\n",
        "\n",
        "## 使用場景\n",
        "\n",
        "程式碼分析是最受歡迎的 LLM 應用程式之一 (例如： [GitHub Co-Pilot](https://github.com/features/copilot), [Code Interpreter](https://chat.openai.com/auth/login?next=%2F%3Fmodel%3Dgpt-4-code-interpreter), [Codium](https://www.codium.ai/), and [Codeium](https://codeium.com/about)) 目前有以下的使用場景:\n",
        "\n",
        "- 對程式碼庫進行問答以了解其工作原理\n",
        "- 使用LLMs提出重構或改進建議\n",
        "- 使用LLMs記錄代碼\n",
        "![Image description](https://python.langchain.com/assets/images/code_understanding-cd1bda63c69e227203a1d5a7e8133887.png)\n",
        "## 概述\n",
        "\n",
        "\n",
        "\n",
        "程式碼分析的問答流程遵循 [我們為文件問答執行的步驟](/docs/extras/use_cases/question_answering), 但有一些差異:\n",
        "\n",
        "特別的是我們可以採用[分而治之的策略](https://python.langchain.com/docs/integrations/document_loaders/source_code)來完成以下幾件事:\n",
        "\n",
        "* 讓程式碼中的每個最頂級函數和類別被載入到單獨的檔案中\n",
        "* 將剩餘部分放入單獨的文件中\n",
        "* 保留有關每個拆分來自何處的元數據\n",
        "\n",
        "## 快速開始"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z1k8f3D9vWBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d19875-9723-4b38-f99d-f4f593bc6394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.4.13-py3-none-any.whl (437 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.8/437.8 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.0.306-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.12)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.103.2-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain)\n",
            "  Downloading langsmith-0.0.41-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Collecting huggingface_hub<0.17,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=d0af69d8d664315ed60a19ba3061ca3da84b8fc9f932be0e7791edb6c24c81b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, websockets, uvloop, python-dotenv, pulsar-client, overrides, mypy-extensions, marshmallow, jsonpointer, humanfriendly, httptools, h11, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, langsmith, jsonpatch, huggingface_hub, coloredlogs, tokenizers, openai, onnxruntime, fastapi, dataclasses-json, langchain, chromadb\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.13 coloredlogs-15.0.1 dataclasses-json-0.6.1 fastapi-0.103.2 h11-0.14.0 httptools-0.6.0 huggingface_hub-0.16.4 humanfriendly-10.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.306 langsmith-0.0.41 marshmallow-3.20.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.16.0 openai-0.28.1 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 tiktoken-0.5.1 tokenizers-0.14.0 typing-inspect-0.9.0 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.20.0 websockets-11.0.3\n",
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.37 smmap-5.0.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install openai tiktoken chromadb langchain\n",
        "!pip install gitpython\n",
        "import os\n",
        "import dotenv\n",
        "os.environ[\"OPENAI_API_KEY\"] =\"這邊要放自己的 OPEN AI API KEY\"\n",
        "# Set env var OPENAI_API_KEY or load from a .env file\n",
        "\n",
        "dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvWjFfbxvWBB"
      },
      "source": [
        "我們將遵循 [本筆記本的結構](https://github.com/cristobalcl/LearningLangChain/blob/master/notebooks/04%20-%20QA%20with%20code.ipynb)並採用 [根據上下文相關性進行的程式碼分割](https://python.langchain.com/docs/integrations/document_loaders/source_code)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TsaUMR0vWBC"
      },
      "source": [
        "### 載入\n",
        "\n",
        "我們將使用`langchain.document_loaders.TextLoader`上傳所有的python project.\n",
        "\n",
        "以下腳本迭代 LangChain 儲存庫中的檔案並載入每個`.py`檔案（又稱文件）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YLt54MfkvWBC"
      },
      "outputs": [],
      "source": [
        "from git import Repo\n",
        "from langchain.text_splitter import Language\n",
        "from langchain.document_loaders.generic import GenericLoader\n",
        "from langchain.document_loaders.parsers import LanguageParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d8-j4lmHvWBD"
      },
      "outputs": [],
      "source": [
        "# Clone\n",
        "repo_path = \"/Users/rlm/Desktop/test_repo\"\n",
        "repo = Repo.clone_from(\"https://github.com/langchain-ai/langchain\", to_path=repo_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnKCiZJzvWBD"
      },
      "source": [
        "\n",
        "我們使用 [`LanguageParser`](https://python.langchain.com/docs/integrations/document_loaders/source_code)加載 py 程式碼，這將：\n",
        "\n",
        "* 將頂級函數和類別放在一起（放入單一文件中）\n",
        "* 將剩餘程式碼放入單獨的文件中\n",
        "* 保留有關每個拆分來自何處的元數據"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L6tz13JRvWBE",
        "outputId": "1d88e8a4-212e-4916-81bb-bcf01263a697",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1546"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Load\n",
        "loader = GenericLoader.from_filesystem(\n",
        "    repo_path+\"/libs/langchain/langchain\",\n",
        "    glob=\"**/*\",\n",
        "    suffixes=[\".py\"],\n",
        "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
        ")\n",
        "documents = loader.load()\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCMeR1EhvWBF"
      },
      "source": [
        "### 分割\n",
        "\n",
        "將其分割`Document` 成區塊以進行嵌入和向量儲存。\n",
        "\n",
        "我們可以使用 `RecursiveCharacterTextSplitter` w/ `language`設定。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3J5DpcRpvWBF",
        "outputId": "7300edc4-3a4d-4dc9-966b-bfb25a9a07be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4695"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON,\n",
        "                                                               chunk_size=2000,\n",
        "                                                               chunk_overlap=200)\n",
        "texts = python_splitter.split_documents(documents)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58FVrg5bvWBF"
      },
      "source": [
        "### RetrievalQA 檢索問答\n",
        "\n",
        "我們需要以一種可以一般文字搜尋其內容的方式儲存文件。\n",
        "\n",
        "最常見的方法是把每個文件的內容文字向量化，然後將嵌入向量和文件儲存在向量存儲中。\n",
        "\n",
        "當設定向量存儲檢索器時：\n",
        "\n",
        "* 我們測試[檢索的最大邊際相關性](/docs/extras/use_cases/question_answering)\n",
        "* 檢索器會傳回8份文件\n",
        "\n",
        "\n",
        "#### 深入學習\n",
        "\n",
        "- [瀏覽超過 40 個向量儲存資料集](https://integrations.langchain.com/)\n",
        "- [有關向量儲存的更多資料](/docs/modules/data_connection/vectorstores/)\n",
        "- [瀏覽超過 30 個文字嵌入資料集](https://integrations.langchain.com/)\n",
        "- [有關嵌入模型的更多資料](/docs/modules/data_connection/text_embedding/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "apkFFWKwvWBG"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"mmr\", # Also test \"similarity\"\n",
        "    search_kwargs={\"k\": 8},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNptNSjavWBG"
      },
      "source": [
        "### 聊天\n",
        "\n",
        "聊天測試，透過[聊天機器人](/docs/extras/use_cases/chatbots)檢索程式碼資訊.\n",
        "\n",
        "#### 深入學習\n",
        "\n",
        "- [在此瀏覽超過55 個 LLM 和聊天模型資料集](https://integrations.langchain.com/)\n",
        "- [有關LLM和聊天模型的更多文件請參閱此處](/docs/modules/model_io/models/)\n",
        "- 使用本地 LLMS： [PrivateGPT](https://github.com/imartinez/privateGPT)和[GPT4All](https://github.com/nomic-ai/gpt4all)的流行反應了本地運行 LLM 的重要性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wYequMxSvWBG"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\")\n",
        "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
        "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nM4ZG3ScvWBG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "e2ba8c87-f5c6-4810-e6c0-18dd13ad47df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The steps to initialize a ReAct agent are as follows:\\n\\n1. Import the necessary modules and classes. This includes the `OpenAI` from `langchain.llms`, `initialize_agent` from `langchain.agents`, `ZapierToolkit` from `langchain.agents.agent_toolkits`, and `ZapierNLAWrapper` from `langchain.utilities.zapier`.\\n\\n2. Create an instance of `OpenAI` with the desired temperature setting. This instance is used as the language model.\\n\\n3. Create an instance of `ZapierNLAWrapper`. This is used to create a toolkit for Zapier, an online automation tool.\\n\\n4. Create an instance of `ZapierToolkit` by calling `from_zapier_nla_wrapper` and passing in the `ZapierNLAWrapper` instance.\\n\\n5. Use the `initialize_agent` function to create the ReAct agent. This function takes in the tools from the `ZapierToolkit`, the `OpenAI` instance, and the type of agent to create (`AgentType.ZERO_SHOT_REACT_DESCRIPTION` in this case).\\n\\n6. Once the agent is created, you can run it using the `run` method and pass in the desired tasks.\\n\\nHere is the corresponding code snippet:\\n\\n```python\\nllm = OpenAI(temperature=0)\\nzapier = ZapierNLAWrapper()\\ntoolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)\\nagent = initialize_agent(\\n    toolkit.get_tools(),\\n    llm,\\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\\n    verbose=True\\n)\\nagent.run((\"Summarize the last email I received regarding Silicon Valley Bank. \"\\n    \"Send the summary to the #test-zapier channel in slack.\"))\\n```\\nThis will initialize and run a ReAct agent that summarizes the last email received regarding Silicon Valley Bank and sends the summary to a specific channel in Slack.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "question = \"How can I initialize a ReAct agent?\"\n",
        "result = qa(question)\n",
        "result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bOFLzMULvWBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e36dac20-c9bc-4526-9891-3f18c3f3285a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> **Question**: What is the class hierarchy? \n",
            "\n",
            "**Answer**: The class hierarchy for initializing a ReAct agent is as follows:\n",
            "\n",
            "- BaseSingleActionAgent\n",
            "    - LLMSingleActionAgent\n",
            "    - OpenAIFunctionsAgent\n",
            "    - XMLAgent\n",
            "    - Agent\n",
            "        - <name>Agent (For example: ZeroShotAgent, ChatAgent, ReActDocstoreAgent)\n",
            "- BaseMultiActionAgent\n",
            "    - OpenAIMultiFunctionsAgent\n",
            "\n",
            "In this hierarchy, ReActDocstoreAgent is a subclass of the Agent class, which itself is a subclass of several classes including the BaseSingleActionAgent. \n",
            "\n",
            "-> **Question**: What classes are derived from the Chain class? \n",
            "\n",
            "**Answer**: The classes that are derived from the Chain class are:\n",
            "\n",
            "1. BaseConversationalRetrievalChain\n",
            "2. ConstitutionalChain\n",
            " \n",
            "\n",
            "-> **Question**: What one improvement do you propose in code in relation to the class herarchy for the Chain class? \n",
            "\n",
            "**Answer**: Based on the provided code, one improvement could be to include more explicit comments or docstrings for each class in the hierarchy. This would make it easier to understand the purpose and functionality of each class, especially for developers who are new to the codebase. For instance, it's not immediately clear what the purpose of classes like `BaseConversationalRetrievalChain` or `SequentialChain` are. Providing a brief explanation of each class in the hierarchy would improve readability and maintainability. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"What is the class hierarchy?\",\n",
        "    \"What classes are derived from the Chain class?\",\n",
        "    \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = qa(question)\n",
        "    print(f\"-> **Question**: {question} \\n\")\n",
        "    print(f\"**Answer**: {result['answer']} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLNE1O2GvWBG"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "我們可以查看[LangSmith](https://smith.langchain.com/public/2b23045f-4e49-4d2d-8980-dec85259af36/r) 追蹤來了解幕後發生的情況：\n",
        "\n",
        "* 特別得是該程式碼會有良好的結構且在檢索結果中整齊地組織在一起。\n",
        "* 檢索到的程式碼和聊天記錄將傳遞給 LLM 進行答案蒸餾\n",
        "\n",
        "![Image description](https://python.langchain.com/assets/images/code_retrieval-238439ab9f5edfe8cbdbc6fcfbc97179.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csbV1yqCvWBH"
      },
      "source": [
        "### 開源 LLMs\n",
        "\n",
        "我們可以透過  LLamaCPP or [Ollama integration](https://ollama.ai/blog/run-code-llama-locally)使用[Code LLaMA](https://about.fb.com/news/2023/08/code-llama-ai-for-coding/)\n",
        "\n",
        "注意：請務必升級`llama-cpp-python`才能使用新的`gguf` [檔案格式](https://github.com/abetlen/llama-cpp-python/pull/633)\n",
        "\n",
        "```\n",
        "CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 /Users/rlm/miniforge3/envs/llama2/bin/pip install -U llama-cpp-python --no-cache-dir\n",
        "```\n",
        "\n",
        "[查看最新的 code-llama 模型](https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/tree/main)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LucGXzr-bDY7",
        "outputId": "ead33d33-9a4c-4cad-8804-6c1c57fd1340"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.11.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.11-cp310-cp310-manylinux_2_35_x86_64.whl size=1023478 sha256=73a14995a5b6878a9cff653d22e0565cf0efa81a57606887ccd3f1d24d759073\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/42/77/a3ab0d02700427ea364de5797786c0272779dce795f62c3bc2\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "A6xAt_7SvWBH"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 下載code llama模型到 colab\n",
        "import requests\n",
        "\n",
        "\n",
        "url = \"https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/e94db8d144152f0b5e153dcb0ac0a266f1588fc3/codellama-13b-instruct.Q4_K_M.gguf\"\n",
        "\n",
        "\n",
        "response = requests.get(url, stream=True)\n",
        "response.raise_for_status()\n",
        "\n",
        "with open(\"codellama-13b-instruct.Q4_K_M.gguf\", \"wb\") as f:\n",
        "    for chunk in response.iter_content(chunk_size=8192):\n",
        "        f.write(chunk)\n",
        "\n",
        "print(\"模型已成功下載！\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QggmeomnexSv",
        "outputId": "90d17851-7216-421d-9e20-5f49f3bf9616"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型已成功下載！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "scrolled": true,
        "id": "8WpqRmi_vWBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7cb0c5-2b0d-4f83-f3ba-c3e9853ecbd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/codellama-13b-instruct.Q4_K_M.gguf\",\n",
        "    n_ctx=5000,\n",
        "    n_gpu_layers=1,\n",
        "    n_batch=512,\n",
        "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "P2CUF1wtvWBH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "c4aee757-95d2-4a04-b891-3414090fe335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " To get a list of all the text files in the current directory that have been modified in the last month.\n",
            "You can use this bash command: find . -type f \\( -iname '*.txt' \\) \\-mtime +30 -print This is because in order to perform this operation in bash, you need to use several different subcommands and options together when invoking the command find.\n",
            "Here is an explanation of each of the main components that make up the bash command that is used to perform this particular operation in bash: . : This is a period symbol that is used as part of the name or path of the file that you want to perform this operation on.\n",
            "Here is an example of how this symbol might be used: .txt : This is a slash symbol that is used as part of the name or path of the file that you want to perform this operation on.\n",
            "Here is an example of how this symbol might be used: dir/ : This is a string literal that contains some text characters.\n",
            "Here is an example of what this particular type of string literal is intended to represent in the context in which it is being used.\n",
            "Note also that there are several different types of quotes that can be used in bash command line"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" To get a list of all the text files in the current directory that have been modified in the last month.\\nYou can use this bash command: find . -type f \\\\( -iname '*.txt' \\\\) \\\\-mtime +30 -print This is because in order to perform this operation in bash, you need to use several different subcommands and options together when invoking the command find.\\nHere is an explanation of each of the main components that make up the bash command that is used to perform this particular operation in bash: . : This is a period symbol that is used as part of the name or path of the file that you want to perform this operation on.\\nHere is an example of how this symbol might be used: .txt : This is a slash symbol that is used as part of the name or path of the file that you want to perform this operation on.\\nHere is an example of how this symbol might be used: dir/ : This is a string literal that contains some text characters.\\nHere is an example of what this particular type of string literal is intended to represent in the context in which it is being used.\\nNote also that there are several different types of quotes that can be used in bash command line\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "llm(\"Question: In bash, how do I list all the text files in the current directory that have been modified in the last month? Answer:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KJTQf6GIvWBH"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LOn6jfdvWBH"
      },
      "source": [
        "由於下列這個功能還處於 close Beta ，我目前沒拿到 LangSmith API 金鑰(close Beta) ，所以這邊我就沒RUN\n",
        "-----------------------------------------------------\n",
        "我們還可以使用 LangChain Prompt Hub 來儲存和取得prompts。\n",
        "\n",
        "這將與您的[LangSmith API 金鑰](https://docs.smith.langchain.com/)一起使用。\n",
        "\n",
        "讓我們在[此處](https://smith.langchain.com/hub/rlm/rag-prompt)嘗試使用預設的 RAG 提示。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchainhub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyyisvL3iOJC",
        "outputId": "96f25def-f127-4aa9-d1fc-8af9fd7d9a7a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.13-py3-none-any.whl (3.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.31.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2023.7.22)\n",
            "Installing collected packages: types-requests, langchainhub\n",
            "Successfully installed langchainhub-0.1.13 types-requests-2.31.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Kt9jeYvbvWBH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "ae5cb0cd-471a-408a-d2fa-6ef9cb2557a8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-30467c838b5b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mQA_CHAIN_PROMPT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rlm/rag-prompt-default\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/hub.py\u001b[0m in \u001b[0;36mpull\u001b[0;34m(owner_repo_commit, api_url, api_key)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \"\"\"\n\u001b[1;32m     79\u001b[0m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mowner_repo_commit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchainhub/client.py\u001b[0m in \u001b[0;36mpull\u001b[0;34m(self, owner_repo_commit)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mowner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_owner_repo_commit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mowner_repo_commit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcommit_hash\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcommit_hash\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"latest\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_latest_commit_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{owner}/{repo}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcommit_hash\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No commits found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchainhub/client.py\u001b[0m in \u001b[0;36m_get_latest_commit_hash\u001b[0;34m(self, repo_full_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_latest_commit_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_full_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mcommits_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_commits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_full_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mcommits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommits_resp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"commits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchainhub/client.py\u001b[0m in \u001b[0;36mlist_commits\u001b[0;34m(self, repo_full_name, limit, offset)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         )\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://api.hub.langchain.com/commits/rlm/rag-prompt-default/?limit=100&offset=0"
          ]
        }
      ],
      "source": [
        "from langchain import hub\n",
        "export LANGCHAIN_TRACING_V2=true\n",
        "export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
        "export LANGCHAIN_API_KEY=<your-api-key>\n",
        "QA_CHAIN_PROMPT = hub.pull(\"rlm/rag-prompt-default\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uzVYbzavWBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18134128-768e-44a1-e08b-d2d0272275b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ],
      "source": [
        "# Docs\n",
        "question = \"How can I initialize a ReAct agent?\"\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "# Chain\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QA_CHAIN_PROMPT)\n",
        "\n",
        "# Run\n",
        "chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whQIHASLvWBI"
      },
      "source": [
        "Here's the trace [RAG](https://smith.langchain.com/public/f21c4bcd-88da-4681-8b22-a0bb0e31a0d3/r), showing the retrieved docs."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}