### 15. **分佈式計算與大數據**
   - 基本的 MapReduce 概念
   - 使用 PySpark 進行大數據處理
   - 熟悉 Hadoop 及其他大數據工具的 Python 接口


### 1. **什麼是 MapReduce？它的基本工作原理是什麼？**
   - **回答：** MapReduce 是一種分佈式計算模型，用於處理大規模數據集。基本工作原理分為兩個階段：`Map` 階段將數據分成鍵值對進行處理，`Reduce` 階段匯總和處理 `Map` 階段的輸出。`Map` 階段通常用於過濾和排序數據，而 `Reduce` 階段則用於匯總結果。

### 2. **在 MapReduce 中，什麼是 Mapper 和 Reducer？**
   - **回答：** `Mapper` 是用於處理和轉換輸入數據的函數，將其分割為鍵值對。`Reducer` 是用於處理 `Mapper` 輸出的鍵值對，進行聚合、總結或其他計算的函數。

### 3. **什麼是 Hadoop？Hadoop 的主要組件有哪些？**
   - **回答：** Hadoop 是一個開源的大數據處理框架，主要用於存儲和處理大數據。Hadoop 的主要組件包括：
     - **HDFS（Hadoop Distributed File System）**：分佈式文件系統，用於存儲大規模數據。
     - **YARN（Yet Another Resource Negotiator）**：資源管理層，用於任務調度和資源管理。
     - **MapReduce**：分佈式計算引擎。

### 4. **什麼是 PySpark？它在大數據處理中的作用是什麼？**
   - **回答：** PySpark 是 Apache Spark 的 Python API，提供了一個基於 Spark 的大數據處理平台，能夠高效地進行大數據分析和處理。它支持分佈式計算，適用於批處理和流處理。

### 5. **如何在 PySpark 中創建一個 SparkContext 對象？**
   - **回答：** 使用 `SparkContext` 類創建 SparkContext 對象，它是 Spark 程序的入口。
     ```python
     from pyspark import SparkContext

     sc = SparkContext(master="local", appName="MyApp")
     ```

### 6. **如何使用 PySpark 讀取一個 CSV 文件並將其轉換為 DataFrame？**
   - **回答：** 使用 PySpark 的 `spark.read.csv()` 方法讀取 CSV 文件並轉換為 DataFrame。
     ```python
     from pyspark.sql import SparkSession

     spark = SparkSession.builder.appName("CSVExample").getOrCreate()
     df = spark.read.csv("data.csv", header=True, inferSchema=True)
     df.show()
     ```

### 7. **什麼是 RDD？在 PySpark 中如何創建一個 RDD？**
   - **回答：** RDD（Resilient Distributed Dataset）是 Spark 的核心數據結構，代表一個不可變的分佈式數據集。在 PySpark 中可以通過並行化集合或從文件中加載數據來創建 RDD。
     ```python
     rdd = sc.parallelize([1, 2, 3, 4, 5])
     ```

### 8. **如何在 PySpark 中對 RDD 進行基本的轉換操作（如 map 和 filter）？**
   - **回答：** 使用 `map()` 和 `filter()` 方法對 RDD 進行轉換操作。
     ```python
     rdd = sc.parallelize([1, 2, 3, 4, 5])
     mapped_rdd = rdd.map(lambda x: x * 2)
     filtered_rdd = rdd.filter(lambda x: x > 2)

     print(mapped_rdd.collect())  # 輸出：[2, 4, 6, 8, 10]
     print(filtered_rdd.collect())  # 輸出：[3, 4, 5]
     ```

### 9. **如何在 PySpark 中使用 SQL 查詢 DataFrame？**
   - **回答：** 使用 `createOrReplaceTempView()` 方法創建臨時表，然後使用 SQL 查詢。
     ```python
     df.createOrReplaceTempView("my_table")
     result_df = spark.sql("SELECT * FROM my_table WHERE age > 30")
     result_df.show()
     ```

### 10. **什麼是 DataFrame 和 Dataset？它們之間有什麼區別？**
   - **回答：** DataFrame 是一種分佈式數據集合，類似於關係數據庫中的表，支持各種 SQL 操作。Dataset 是一種強類型的 DataFrame，支持編譯時類型檢查和對象操作。Dataset 在 Python API 中通常表示為 DataFrame。

### 11. **如何在 PySpark 中進行數據聚合操作？**
   - **回答：** 使用 `groupBy()` 和聚合函數（如 `sum()`、`avg()`）進行數據聚合。
     ```python
     df.groupBy("category").sum("value").show()
     ```

### 12. **什麼是 Spark Streaming？它的作用是什麼？**
   - **回答：** Spark Streaming 是 Spark 提供的實時數據流處理框架。它能夠處理實時數據流（如來自 Kafka、Flume、TCP 套接字）的數據，並將結果寫回到 HDFS、數據庫或實時儀表板。

### 13. **如何在 PySpark 中實現簡單的 WordCount 示例？**
   - **回答：** 使用 `flatMap()` 和 `reduceByKey()` 方法計算單詞出現的次數。
     ```python
     text_rdd = sc.textFile("text.txt")
     word_counts = text_rdd.flatMap(lambda line: line.split(" ")) \
                           .map(lambda word: (word, 1)) \
                           .reduceByKey(lambda a, b: a + b)
     print(word_counts.collect())
     ```

### 14. **什麼是 Hadoop 的 HDFS？如何使用 Python 接口讀取 HDFS 中的文件？**
   - **回答：** HDFS 是 Hadoop 分佈式文件系統，用於存儲大規模數據集。可以使用 `hadoop fs` 命令或者 Python 的 `hdfs` 庫讀取 HDFS 文件。
     ```bash
     hadoop fs -cat /path/to/file.txt
     ```
     使用 `hdfs` 庫：
     ```python
     from hdfs import InsecureClient

     client = InsecureClient('http://namenode_host:50070')
     with client.read('/path/to/file.txt') as reader:
         content = reader.read()
         print(content)
     ```

### 15. **如何在 PySpark 中處理缺失值？**
   - **回答：** 使用 `na.fill()` 或 `na.drop()` 方法處理缺失值。
     ```python
     df.na.fill(0).show()  # 用 0 填充缺失值
     df.na.drop().show()   # 刪除包含缺失值的行
     ```

這些問題涵蓋了 Python 分佈式計算與大數據中的關鍵概念，包括 MapReduce 基本原理、使用 PySpark 進行大數據處理、以及熟悉 Hadoop 及其他大數據工具的 Python 接口。熟悉這些問題和答案有助於在面試中展示對大數據和分佈式計算技術的理解和應用。