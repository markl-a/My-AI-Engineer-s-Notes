{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ddY6fGrv3QYV"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries.\n",
        "import os\n",
        "\n",
        "# Clone the repository.\n",
        "os.system(\"git clone https://github.com/markl-a/LLM_relatedDemo.git\")\n",
        "\n",
        "# Change to the directory containing the code.\n",
        "os.chdir(\"/content/LLM_relatedDemo/GaLore_Demo/GaLore-master\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmbYp3ap0-j-",
        "outputId": "6fcd3327-33d5-47df-c1cf-06f5e0388986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/LLM_relatedDemo/experiment/GaLore-master\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from galore-torch==1.0) (2.1.0+cu121)\n",
            "Collecting transformers==4.31.0 (from galore-torch==1.0)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from galore-torch==1.0) (0.15.2)\n",
            "Collecting datasets (from galore-torch==1.0)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft (from galore-torch==1.0)\n",
            "  Downloading peft-0.9.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb (from galore-torch==1.0)\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loguru (from galore-torch==1.0)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvitop (from galore-torch==1.0)\n",
            "  Downloading nvitop-1.3.2-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.4/215.4 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lion-pytorch (from galore-torch==1.0)\n",
            "  Downloading lion_pytorch-0.1.2-py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from galore-torch==1.0) (3.7.1)\n",
            "Collecting bitsandbytes (from galore-torch==1.0)\n",
            "  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from galore-torch==1.0) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from galore-torch==1.0) (1.2.2)\n",
            "Collecting evaluate (from galore-torch==1.0)\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->galore-torch==1.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->galore-torch==1.0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->galore-torch==1.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->galore-torch==1.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->galore-torch==1.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->galore-torch==1.0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->galore-torch==1.0) (2.31.0)\n",
            "Collecting tokenizers (from galore-torch==1.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->galore-torch==1.0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->galore-torch==1.0) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->galore-torch==1.0) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->galore-torch==1.0) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->galore-torch==1.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->galore-torch==1.0) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->galore-torch==1.0) (3.4.1)\n",
            "Collecting multiprocess (from datasets->galore-torch==1.0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->galore-torch==1.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->galore-torch==1.0) (3.9.3)\n",
            "Collecting responses<0.19 (from evaluate->galore-torch==1.0)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->galore-torch==1.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->galore-torch==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->galore-torch==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->galore-torch==1.0) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->galore-torch==1.0) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->galore-torch==1.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->galore-torch==1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->galore-torch==1.0) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->galore-torch==1.0) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->galore-torch==1.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->galore-torch==1.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->galore-torch==1.0) (2.8.2)\n",
            "Collecting nvidia-ml-py<12.536.0a0,>=11.450.51 (from nvitop->galore-torch==1.0)\n",
            "  Downloading nvidia_ml_py-12.535.133-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.10/dist-packages (from nvitop->galore-torch==1.0) (5.9.5)\n",
            "Requirement already satisfied: cachetools>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from nvitop->galore-torch==1.0) (5.3.3)\n",
            "Requirement already satisfied: termcolor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nvitop->galore-torch==1.0) (2.4.0)\n",
            "Collecting accelerate>=0.21.0 (from peft->galore-torch==1.0)\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->galore-torch==1.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->galore-torch==1.0) (3.3.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->galore-torch==1.0) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->galore-torch==1.0)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb->galore-torch==1.0)\n",
            "  Downloading sentry_sdk-1.41.0-py2.py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.8/258.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->galore-torch==1.0)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb->galore-torch==1.0)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->galore-torch==1.0) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->galore-torch==1.0) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->galore-torch==1.0) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->galore-torch==1.0) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->galore-torch==1.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->galore-torch==1.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->galore-torch==1.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->galore-torch==1.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->galore-torch==1.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->galore-torch==1.0) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->galore-torch==1.0)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0->galore-torch==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0->galore-torch==1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0->galore-torch==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0->galore-torch==1.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->galore-torch==1.0) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->galore-torch==1.0) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->galore-torch==1.0) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->galore-torch==1.0)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: tokenizers, nvidia-ml-py, smmap, setproctitle, sentry-sdk, nvitop, loguru, docker-pycreds, dill, responses, multiprocess, gitdb, transformers, lion-pytorch, GitPython, bitsandbytes, accelerate, wandb, peft, datasets, evaluate, galore-torch\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.2\n",
            "    Uninstalling transformers-4.38.2:\n",
            "      Successfully uninstalled transformers-4.38.2\n",
            "  Running setup.py develop for galore-torch\n",
            "Successfully installed GitPython-3.1.42 accelerate-0.27.2 bitsandbytes-0.43.0 datasets-2.18.0 dill-0.3.8 docker-pycreds-0.4.0 evaluate-0.4.1 galore-torch-1.0 gitdb-4.0.11 lion-pytorch-0.1.2 loguru-0.7.2 multiprocess-0.70.16 nvidia-ml-py-12.535.133 nvitop-1.3.2 peft-0.9.0 responses-0.18.0 sentry-sdk-1.41.0 setproctitle-1.3.3 smmap-5.0.1 tokenizers-0.13.3 transformers-4.31.0 wandb-0.16.4\n"
          ]
        }
      ],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OGk3UzW_D-F"
      },
      "source": [
        "用法\n",
        "\n",
        "\n",
        "```\n",
        "from galore_torch import GaLoreAdamW, GaLoreAdamW8bit, GaLoreAdafactor\n",
        "# define param groups as galore_params and non_galore_params\n",
        "param_groups = [{'params': non_galore_params},\n",
        "                {'params': galore_params, 'rank': 128, 'update_proj_gap': 200, 'scale': 0.25, 'proj_type': 'std'}]\n",
        "optimizer = GaLoreAdamW(param_groups, lr=0.01)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRvbs1wD_rDH"
      },
      "source": [
        "請先登入 wandb : 請參考 [wandb使用教學](https://zhuanlan.zhihu.com/p/493093033)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I29n1qIM66jk",
        "outputId": "137913e7-b756-4698-fd6c-4ae9d53d72ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gi1Xs4f-BR1"
      },
      "source": [
        "# 使用具有 24GB 記憶體的單一 GPU 訓練 7B 模型 (這邊在colab 是使用A100 )\n",
        "\n",
        "要使用單一 GPU（例如 NVIDIA RTX 4090）訓練 7B 模型，您所需要做的就是指定--optimizer=galore_adamw8bit_per_layer，這會啟用GaLoreAdamW8bit每層權重更新。透過啟動檢查點，您可以將在 NVIDIA RTX 4090 上測試的批次大小保持為 16。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xM8JW3O9aDZ",
        "outputId": "37d0bc26-71f0-4a1a-c9ef-8dea63cbd2f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-03-11 14:42:22,628] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\n",
            "Starting script\n",
            "\u001b[32m2024-03-11 14:42:29.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mGlobal rank 0, local rank 0, device: 0\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:29.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mProcess group initialized\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mm4932981\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLM_relatedDemo/experiment/GaLore-master/wandb/run-20240311_144230-wibzfpu6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mserene-firebrand-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/m4932981/galore-c4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/m4932981/galore-c4/runs/wibzfpu6\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mUsing dist with rank 0 (only rank 0 will log)\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m164\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mStarting training with the arguments\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mmodel_config                   configs/llama_7b.json\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1muse_hf_model                   False\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mcontinue_from                  None\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mbatch_size                     16\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mgradient_accumulation          32\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mtotal_batch_size               512\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mmax_length                     256\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1moptimizer                      galore_adamw8bit_per_layer\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mlr                             0.005\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mscheduler                      cosine\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mmin_lr_ratio                   0.1\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mactivation_checkpointing       True\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mweight_decay                   0.0\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mwarmup_steps                   15000\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1meval_every                     1000\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mnum_training_steps             150000\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mmax_train_tokens               None\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1msave_every                     10000\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1msave_dir                       checkpoints/llama_7b-2024-03-11-14-42-29\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mtags                           None\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mdtype                          bfloat16\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mworkers                        8\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mseed                           0\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mname                           test\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mgrad_clipping                  1.0\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mbeta1                          0.0\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mrank                           1024\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mupdate_proj_gap                500\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mgalore_scale                   0.25\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mproj_type                      std\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1msingle_gpu                     True\u001b[0m\n",
            "\u001b[32m2024-03-11 14:42:31.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m168\u001b[0m - \u001b[1m****************************************\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for c4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/c4\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Downloading builder script: 100% 3.46k/3.46k [00:00<00:00, 11.2MB/s]\n",
            "Downloading readme: 100% 8.21k/8.21k [00:00<00:00, 19.4MB/s]\n",
            "/root/.cache/huggingface/modules/datasets_modules/datasets/c4/584d57ebe81c209b6c7f31727066d2c4b4bba37cb7092cdd83083d5ec11207db/c4.py:53: FutureWarning: Dataset 'c4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
            "  warnings.warn(\n",
            "\u001b[32m2024-03-11 14:42:36.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m174\u001b[0m - \u001b[1mShuffling data with seed 42\u001b[0m\n",
            "config.json: 100% 1.21k/1.21k [00:00<00:00, 3.83MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 1.59MB/s]\n",
            "tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 1.41MB/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "Update steps:   0%|                                  | 0/150000 [00:00<?, ?it/s]enable GaLore for weights in module:  model.layers.0.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.0.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.0.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.0.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.0.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.0.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.0.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.1.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.1.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.1.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.1.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.1.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.1.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.1.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.2.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.2.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.2.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.2.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.2.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.2.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.2.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.3.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.3.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.3.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.3.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.3.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.3.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.3.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.4.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.4.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.4.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.4.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.4.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.4.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.4.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.5.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.5.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.5.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.5.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.5.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.5.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.5.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.6.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.6.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.6.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.6.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.6.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.6.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.6.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.7.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.7.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.7.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.7.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.7.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.7.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.7.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.8.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.8.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.8.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.8.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.8.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.8.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.8.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.9.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.9.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.9.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.9.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.9.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.9.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.9.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.10.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.10.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.10.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.10.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.10.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.10.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.10.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.11.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.11.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.11.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.11.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.11.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.11.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.11.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.12.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.12.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.12.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.12.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.12.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.12.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.12.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.13.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.13.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.13.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.13.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.13.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.13.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.13.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.14.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.14.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.14.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.14.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.14.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.14.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.14.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.15.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.15.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.15.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.15.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.15.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.15.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.15.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.16.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.16.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.16.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.16.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.16.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.16.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.16.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.17.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.17.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.17.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.17.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.17.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.17.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.17.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.18.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.18.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.18.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.18.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.18.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.18.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.18.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.19.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.19.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.19.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.19.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.19.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.19.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.19.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.20.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.20.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.20.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.20.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.20.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.20.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.20.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.21.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.21.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.21.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.21.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.21.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.21.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.21.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.22.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.22.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.22.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.22.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.22.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.22.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.22.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.23.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.23.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.23.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.23.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.23.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.23.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.23.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.24.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.24.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.24.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.24.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.24.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.24.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.24.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.25.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.25.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.25.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.25.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.25.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.25.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.25.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.26.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.26.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.26.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.26.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.26.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.26.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.26.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.27.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.27.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.27.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.27.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.27.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.27.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.27.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.28.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.28.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.28.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.28.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.28.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.28.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.28.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.29.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.29.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.29.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.29.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.29.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.29.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.29.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.30.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.30.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.30.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.30.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.30.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.30.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.30.mlp.up_proj\n",
            "enable GaLore for weights in module:  model.layers.31.self_attn.q_proj\n",
            "enable GaLore for weights in module:  model.layers.31.self_attn.k_proj\n",
            "enable GaLore for weights in module:  model.layers.31.self_attn.v_proj\n",
            "enable GaLore for weights in module:  model.layers.31.self_attn.o_proj\n",
            "enable GaLore for weights in module:  model.layers.31.mlp.gate_proj\n",
            "enable GaLore for weights in module:  model.layers.31.mlp.down_proj\n",
            "enable GaLore for weights in module:  model.layers.31.mlp.up_proj\n",
            "\u001b[32m2024-03-11 14:44:26.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1m\n",
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
            "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n",
            "\u001b[0m\n",
            "\u001b[32m2024-03-11 14:44:26.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m285\u001b[0m - \u001b[1mTotal params: 6738.42M\u001b[0m\n",
            "\u001b[32m2024-03-11 14:44:26.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m286\u001b[0m - \u001b[1mTrainable params: 6738.42M\u001b[0m\n",
            "\u001b[32m2024-03-11 14:44:26.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mTotal params with GaLore enabled: 6476.01M\u001b[0m\n",
            "\u001b[32m2024-03-11 14:44:26.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1mSaving model to checkpoints/llama_7b-2024-03-11-14-42-29 every 10000 update steps\u001b[0m\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "Update steps:   0%|                     | 7/150000 [14:38<2797:17:33, 67.14s/it]"
          ]
        }
      ],
      "source": [
        "# LLaMA-7B, 8-bit GaLore-Adam, single GPU, activation checkpointing\n",
        "# bsz=16, 22.8G,\n",
        "!torchrun --standalone --nproc_per_node 1 torchrun_main.py \\\n",
        "    --model_config configs/llama_7b.json \\\n",
        "    --lr 0.005 \\\n",
        "    --galore_scale 0.25 \\\n",
        "    --rank 1024 \\\n",
        "    --update_proj_gap 500 \\\n",
        "    --batch_size 16 \\\n",
        "    --total_batch_size 512 \\\n",
        "    --activation_checkpointing \\\n",
        "    --num_training_steps 150000 \\\n",
        "    --warmup_steps 15000 \\\n",
        "    --weight_decay 0 \\\n",
        "    --grad_clipping 1.0 \\\n",
        "    --dtype bfloat16 \\\n",
        "    --eval_every 1000 \\\n",
        "    --single_gpu \\\n",
        "    --optimizer galore_adamw8bit_per_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaNS_37y-Q1J"
      },
      "source": [
        "# **Benchmark 1：在C4資料集上預訓練LLaMA**\n",
        "\n",
        "torchrun_main.py是使用 GaLore 在 C4 上訓練 LLaMA 模型的主要腳本。我們針對各種尺寸模型的基準腳本位於scripts/benchmark_c4資料夾中。例如，要在 C4 上訓練 60m 模型，請執行以下操作："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnKwpk2V4SjZ"
      },
      "outputs": [],
      "source": [
        "# LLaMA-60M, GaLore-Adam, 1 A100, 1 Node\n",
        "!torchrun --standalone --nproc_per_node 1 torchrun_main.py \\\n",
        "    --model_config configs/llama_60m.json \\\n",
        "    --lr 0.01 \\\n",
        "    --galore_scale 0.25 \\\n",
        "    --rank 128 \\\n",
        "    --update_proj_gap 200 \\\n",
        "    --batch_size 256 \\\n",
        "    --total_batch_size 512 \\\n",
        "    --num_training_steps 10000 \\\n",
        "    --warmup_steps 1000 \\\n",
        "    --weight_decay 0 \\\n",
        "    --dtype bfloat16 \\\n",
        "    --eval_every 1000 \\\n",
        "    --optimizer galore_adamw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn5_Ux3w-mIr"
      },
      "source": [
        "# **Benchmark 2：在 GLUE 任務上微調 RoBERTa**\n",
        "\n",
        "run_glue.py是使用 GaLore 在 GLUE 任務上微調 RoBERTa 模型的主要腳本。範例腳本如下圖所示："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yom1v-Er-0K4"
      },
      "outputs": [],
      "source": [
        "!python run_glue.py \\\n",
        "    --model_name_or_path roberta-base \\\n",
        "    --task_name mrpc \\\n",
        "    --enable_galore \\\n",
        "    --lora_all_modules \\\n",
        "    --max_length 512 \\\n",
        "    --seed=1234 \\\n",
        "    --lora_r 4 \\\n",
        "    --galore_scale 4 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --update_proj_gap 500 \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 30 \\\n",
        "    --output_dir results/ft/roberta_base/mrpc"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
